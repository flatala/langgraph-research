{
    "reasoning": "I first catalogued 18 post-2023 arXiv papers retrieved with the user-supplied queries, then filtered to 16 that are directly relevant to user-specific personalisation and conditional/value alignment of LLMs. To minimise duplication, I allocated each paper to one primary section, reusing a subset only when the comparison or synthesis explicitly requires it (justified in citation_reason). The \u2018Landscape\u2019 section groups methods thematically (light-weight personalisation, personalised RLHF, multi-objective / conditional alignment). The \u2018Synthesis\u2019 section re-uses benchmark and safety papers to critique evaluation gaps and trade-offs. All papers are cited at least once, citations are distributed evenly, and the progression of sections (conceptual background \u2192 methods \u2192 critical discussion \u2192 conclusion \u2192 future work) yields a logically flowing review.",
    "plan": [
        {
            "number": 1,
            "title": "Introduction",
            "outline": "Define personalisation and conditional alignment for LLMs, motivate their importance for helpful, harmless, user-specific behaviour, and delimit the scope of the review.",
            "key_points": [
                {
                    "text": "The growth of user-centric applications is driving demand for LLMs that adapt to individual profiles, contexts, and devices.",
                    "papers": [
                        {
                            "title": "PLMM: Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Proposes a three-tier architecture and distillation pipeline that yields small, on-device personal LLMs while preserving privacy.",
                            "citation_reason": "Demonstrates practical motivation for personalised models and highlights privacy/device constraints introduced in the introduction."
                        },
                        {
                            "title": "LaMP: When Large Language Models Meet Personalization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2304.11406",
                            "summary": "Introduces the LaMP benchmark covering seven tasks to train and evaluate personalised text classification and generation.",
                            "citation_reason": "Provides evidence of rising interest and offers early benchmark context for defining the field\u2019s scope."
                        },
                        {
                            "title": "Everyone Deserves A Reward: Learning Customized Human Preferences",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2309.03126",
                            "summary": "Presents a domain-specific preference dataset and a three-stage scheme for customised reward modelling.",
                            "citation_reason": "Illustrates the diversity of individual user preferences, underscoring the review\u2019s focus on user-level alignment."
                        }
                    ]
                },
                {
                    "text": "Ensuring value alignment under diverse preferences introduces fresh challenges around safety, controllability, and evaluation.",
                    "papers": [
                        {
                            "title": "ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2312.02554",
                            "summary": "Proposes Point-wise DPO and a single-step procedure unifying supervised fine-tuning with point-wise preference learning.",
                            "citation_reason": "Shows why new alignment objectives are required beyond traditional pairwise RLHF, motivating conditional alignment."
                        },
                        {
                            "title": "Parameter-Efficient Tuning Helps Language Model Alignment",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2310.00819",
                            "summary": "Introduces MEET, using parameter-efficient control tokens for multi-preference controllable generation.",
                            "citation_reason": "Highlights controllability as a core alignment concern introduced here and elaborated later."
                        },
                        {
                            "title": "Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.05466",
                            "summary": "Presents a benchmark exposing models that pretend to be aligned but misbehave when unsupervised.",
                            "citation_reason": "Frames the risk of superficial alignment, reinforcing the importance of trustworthy conditional alignment."
                        }
                    ]
                }
            ]
        },
        {
            "number": 2,
            "title": "Thematic and Methodological Landscape",
            "outline": "Map recent research into three dominant methodological strata: light-weight personalisation, personalised RLHF, and multi-objective or cross-context conditional alignment.",
            "key_points": [
                {
                    "text": "Light-weight or prompt-level techniques enable rapid personality or identity conditioning without full model retraining.",
                    "papers": [
                        {
                            "title": "Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2310.16582",
                            "summary": "Uses unsupervised personalised lexicons applied during decoding to steer fine-grained personality expression.",
                            "citation_reason": "Represents lexicon-based decoding as a minimal-overhead personalisation paradigm."
                        },
                        {
                            "title": "Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2411.10006",
                            "summary": "Introduces personality-conditioned instruction tuning and a new OrcaBench for role-playing evaluation.",
                            "citation_reason": "Showcases instruction-tuning with psychological traits, typifying prompt-centric adaptation."
                        },
                        {
                            "title": "Personalized Visual Instruction Tuning",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2410.07113",
                            "summary": "Curates synthetic personalised conversations to fine-tune multimodal LLMs that recognise and converse about specific individuals.",
                            "citation_reason": "Extends light-weight personalisation to multimodal settings, broadening the landscape theme."
                        }
                    ]
                },
                {
                    "text": "Personalised RLHF frameworks learn user-specific reward functions while addressing data scarcity and privacy.",
                    "papers": [
                        {
                            "title": "A Shared Low-Rank Adaptation Approach to Personalized RLHF",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2503.19201",
                            "summary": "Uses LoRA in a shared parameter space to learn personalised reward models efficiently with theoretical guarantees.",
                            "citation_reason": "Illustrates parameter-sharing for scalable personalised RLHF under limited data."
                        },
                        {
                            "title": "FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2412.15538",
                            "summary": "Introduces federated RLHF, enabling local integration of human feedback without sharing raw data and providing convergence proofs.",
                            "citation_reason": "Represents privacy-preserving personalisation, filling an important sub-theme."
                        },
                        {
                            "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2504.03206",
                            "summary": "Adds curiosity-based intrinsic rewards so agents actively infer user traits during RLHF, improving personalisation in dialogue.",
                            "citation_reason": "Shows novel reward design aimed specifically at increasing personalisation depth."
                        }
                    ]
                },
                {
                    "text": "Multi-objective and cross-context alignment methods strive for generalisable conditioning across tasks, languages, and personalities.",
                    "papers": [
                        {
                            "title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2403.17141",
                            "summary": "Presents a policy-agnostic three-stage pipeline that reformulates objectives and performs plug-and-play multi-objective alignment.",
                            "citation_reason": "Epitomises scalable conditional alignment across heterogeneous objectives."
                        },
                        {
                            "title": "Concept Space Alignment in Multilingual LLMs",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2410.01079",
                            "summary": "Empirically studies linear concept alignment across languages, showing typological and abstraction effects.",
                            "citation_reason": "Offers evidence on cross-lingual alignment, broadening conditional alignment beyond user traits."
                        },
                        {
                            "title": "Dynamic Generation of Personalities with Large Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2404.07084",
                            "summary": "Introduces hypernetwork-based dynamic personality generation and an automatic personality evaluation metric.",
                            "citation_reason": "Demonstrates dynamic, on-the-fly personality conditioning, complementing multi-objective alignment studies."
                        }
                    ]
                }
            ]
        },
        {
            "number": 3,
            "title": "Synthesis & Critical Discussion",
            "outline": "Compare methodologies, surface common patterns, contradictions, and methodological limitations across the literature.",
            "key_points": [
                {
                    "text": "Benchmarking remains fragmented, with limited measures for real-world personalisation quality and alignment robustness.",
                    "papers": [
                        {
                            "title": "Large Language Models Empowered Personalized Web Agents",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2410.17236",
                            "summary": "Creates PersonalWAB benchmark and proposes PUMA for memory-based personalised Web agents.",
                            "citation_reason": "Provides the most recent, large-scale benchmark explicitly targeting personalised agent evaluation."
                        },
                        {
                            "title": "Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.05466",
                            "summary": "Shows that models can fake alignment and proposes detection strategies.",
                            "citation_reason": "Highlights evaluation blind spots\u2014used again here to critique robustness gaps."
                        },
                        {
                            "title": "LaMP: When Large Language Models Meet Personalization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2304.11406",
                            "summary": "Introduces a multi-task personalisation benchmark with retrieval augmentation baselines.",
                            "citation_reason": "Offers comparative evidence of metric variability across tasks, reinforcing benchmarking challenges."
                        }
                    ]
                },
                {
                    "text": "Personalisation may conflict with global safety aims; balancing user control, privacy, and alignment fidelity remains unresolved.",
                    "papers": [
                        {
                            "title": "A Shared Low-Rank Adaptation Approach to Personalized RLHF",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2503.19201",
                            "summary": "LoRA-based personalised reward learning balances shared structure and individual adaptation.",
                            "citation_reason": "Provides quantitative insight into trade-offs between general and user-specific reward models."
                        },
                        {
                            "title": "ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2312.02554",
                            "summary": "Single-step alignment unifies demonstrations and point-wise feedback.",
                            "citation_reason": "Contrasts global point-wise alignment with individual-level RLHF, enabling critical discussion of safety vs. personalisation."
                        }
                    ]
                }
            ]
        },
        {
            "number": 4,
            "title": "Conclusion",
            "outline": "Summarise consolidated insights, highlighting what is now well-understood about personalisation and conditional alignment of LLMs.",
            "key_points": [
                {
                    "text": "Light-weight personalisation techniques achieve convincing style and persona control, yet still rely on constrained benchmarks and limited domain coverage.",
                    "papers": [
                        {
                            "title": "Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2310.16582",
                            "summary": "Uses lexicons to manipulate personality expressions at decoding time.",
                            "citation_reason": "Summarises successes and constraints of prompt-level methods in the concluding synthesis."
                        },
                        {
                            "title": "ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2312.02554",
                            "summary": "Point-wise preference alignment improves controllability without heavy RLHF.",
                            "citation_reason": "Helps conclude that alignment quality is improving but still restricted by feedback formats."
                        }
                    ]
                },
                {
                    "text": "Conditional multi-objective alignment methods generalise across tasks and languages, yet remain sensitive to unseen objectives and adversarial contexts.",
                    "papers": [
                        {
                            "title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2403.17141",
                            "summary": "Achieves plug-and-play alignment across unseen objectives with substantial GPU savings.",
                            "citation_reason": "Provides evidence that generalisable alignment is feasible but still task-dependent."
                        },
                        {
                            "title": "Concept Space Alignment in Multilingual LLMs",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2410.01079",
                            "summary": "Examines linear alignments between languages, revealing typological limits.",
                            "citation_reason": "Reinforces conclusion on context-specific alignment fragility across languages."
                        }
                    ]
                }
            ]
        },
        {
            "number": 5,
            "title": "Future Directions",
            "outline": "Identify research gaps and propose opportunities for advancing personalisation and conditional alignment of LLMs.",
            "key_points": [
                {
                    "text": "On-device, privacy-preserving alignment pipelines and federated RLHF are needed to democratise personalised AI safely.",
                    "papers": [
                        {
                            "title": "PLMM: Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Shows feasibility of running distilled personal LLMs locally with encrypted inputs.",
                            "citation_reason": "Illustrates the technical foundation upon which future privacy-preserving personalisation research can build."
                        },
                        {
                            "title": "FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2412.15538",
                            "summary": "Offers federated RLHF with theoretical guarantees, avoiding raw data sharing.",
                            "citation_reason": "Suggests a promising direction for aligning models without compromising user data."
                        }
                    ]
                },
                {
                    "text": "Standardised, adversarial benchmarks and detection tools are essential to prevent alignment fakery and objectively measure personalised performance.",
                    "papers": [
                        {
                            "title": "Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.05466",
                            "summary": "Creates a benchmark to detect models that feign alignment.",
                            "citation_reason": "Indicates the need for adversarial testing frameworks in future evaluation suites."
                        },
                        {
                            "title": "Large Language Models Empowered Personalized Web Agents",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2410.17236",
                            "summary": "Proposes PersonalWAB benchmark for personalised Web agents and evaluates PUMA.",
                            "citation_reason": "Shows how domain-specific benchmarks can evolve into broader standardised evaluation ecosystems."
                        }
                    ]
                }
            ]
        }
    ]
}