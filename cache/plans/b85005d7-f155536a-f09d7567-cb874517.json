{
    "reasoning": "The review is structured to move from broad motivation and definitions to detailed methodological advances, critical synthesis, conclusions, and forward-looking research gaps. Section 1 establishes the importance of personalising LLM alignment and introduces ethical and evaluative contexts using six conceptual or early-practice papers. Section 2 maps the methodological terrain, grouping recent work into four dominant themes (context-aware RLHF, multi-objective preference modelling, persona/attribute conditioning, and continual personalisation) and citing nine distinct technical papers. Section 3 critically compares evaluation practices, forgetting\u2013generalisation trade-offs, and fairness issues, deliberately re-using three papers (clearly justified) alongside three fresh ones to anchor the discussion. Section 4 summarises converging insights, while Section 5 projects future directions, each drawing on previously cited papers when essential to demonstrate continuity. All 18 post-2023 papers are cited at least once; duplication is limited and always explained in \"citation_reason\".",
    "plan": [
        {
            "number": 1,
            "title": "Introduction: Why Personalised and Conditional Alignment Matters",
            "outline": "Define personalised alignment of LLMs, distinguish it from generic alignment, and frame its societal and technical significance.",
            "key_points": [
                {
                    "text": "Rapid emergence of RLHF frameworks explicitly designed for user-level personalisation shows a paradigm shift from one-size-fits-all alignment.",
                    "papers": [
                        {
                            "title": "Personalized Language Modeling from Personalized Human Feedback",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.05133",
                            "summary": "Introduces P-RLHF, a framework that jointly learns user models and LLM policies to generate preference-aligned responses without requiring fully articulated prompts.",
                            "citation_reason": "Illustrates the core idea of tailoring RLHF to individual users, motivating the review."
                        },
                        {
                            "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2404.00934",
                            "summary": "Describes an industry-scale RLHF pipeline for ChatGLM, detailing data collection, reward modelling, and policy optimisation challenges.",
                            "citation_reason": "Provides evidence that personalisation-aware alignment is already deployed at production scale."
                        }
                    ]
                },
                {
                    "text": "Ethical, policy, and organisational considerations highlight risks and opportunities unique to personalised alignment.",
                    "papers": [
                        {
                            "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2303.05453",
                            "summary": "Offers a taxonomy of benefits and risks of personalised LLMs and proposes a three-tier policy framework to bound unsafe behaviour.",
                            "citation_reason": "Supplies normative vocabulary for framing subsequent technical discussions."
                        },
                        {
                            "title": "Personality of AI",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2312.02998",
                            "summary": "Explores the concept of \"personality alignment\" for organisational roles and raises questions about evaluating AI personalities.",
                            "citation_reason": "Broadens the introduction beyond technical aspects to organisational context."
                        }
                    ]
                },
                {
                    "text": "Reliable measurement of persona consistency and user-specific effects is increasingly recognised as a foundational requirement.",
                    "papers": [
                        {
                            "title": "Quantifying the Persona Effect in LLM Simulations",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.10811",
                            "summary": "Shows that persona variables explain limited variance in subjective NLP datasets yet improve LLM simulations when explicitly prompted.",
                            "citation_reason": "Demonstrates why rigorous evaluation metrics are crucial for personalised alignment."
                        },
                        {
                            "title": "PersonaGym: Evaluating Persona Agents and LLMs",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2407.18416",
                            "summary": "Introduces a dynamic benchmark and automatic metric (PersonaScore) for assessing persona fidelity across thousands of scenarios.",
                            "citation_reason": "Provides a large-scale evaluation framework emphasising the measurement gap identified in this point."
                        }
                    ]
                }
            ]
        },
        {
            "number": 2,
            "title": "Methodological Landscape: Approaches to Personalised and Conditional Alignment",
            "outline": "Survey and group recent methods into four dominant technical themes, highlighting design choices and trends.",
            "key_points": [
                {
                    "text": "Context-aware RLHF methods incorporate explicit user or contextual embeddings to personalise reward modelling and policy learning.",
                    "papers": [
                        {
                            "title": "Low-Rank Contextual Reinforcement Learning from Heterogeneous Human Feedback",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2412.19436",
                            "summary": "Proposes LoCo-RLHF, using low-rank structures to model user-context interactions and mitigate distribution shifts in feedback.",
                            "citation_reason": "Representative of lightweight, scalable user-conditioned RLHF techniques."
                        },
                        {
                            "title": "Hybrid Preference Optimization for Alignment: Provably Faster Convergence Rates by Combining Offline Preferences with Online Exploration",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2412.10616",
                            "summary": "Introduces HPO, a hybrid offline\u2013online algorithm with optimal sample-efficiency guarantees for preference-based alignment.",
                            "citation_reason": "Shows how contextual RLHF can be blended with active exploration for efficiency."
                        }
                    ]
                },
                {
                    "text": "Multi-objective and direction-based reward modelling enables arithmetic control over diverse user preferences.",
                    "papers": [
                        {
                            "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.18571",
                            "summary": "Presents DPA, representing user preferences as vectors in reward space to support intuitive trade-off control via vector arithmetic.",
                            "citation_reason": "Embodies state-of-the-art multi-objective alignment techniques."
                        },
                        {
                            "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.16455",
                            "summary": "Identifies preference-collapse under KL regularisation and proposes Preference-Matching RLHF to align with heterogeneous distributions.",
                            "citation_reason": "Highlights both a methodological pitfall and a corrective strategy in reward modelling."
                        }
                    ]
                },
                {
                    "text": "Persona and attribute conditioning leverage latent-space or data-augmented mechanisms for fine-grained style and behaviour control.",
                    "papers": [
                        {
                            "title": "MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2310.18342",
                            "summary": "Disentangles multi-facet personality attributes in a latent space using energy-based models for controllable dialogue generation.",
                            "citation_reason": "Demonstrates attribute-level controllability beyond simple profile text."
                        },
                        {
                            "title": "Dialogue Language Model with Large-Scale Persona Data Engineering",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2412.09034",
                            "summary": "Builds a large persona dialogue dataset via automated extraction and augmentation to pre-train persona-consistent dialogue LLMs.",
                            "citation_reason": "Shows the importance of data engineering for persona consistency."
                        },
                        {
                            "title": "ULMRec: User-centric Large Language Model for Sequential Recommendation",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2412.05543",
                            "summary": "Integrates vector-quantised user indexing and alignment tuning to infuse personalised preferences into recommendation-oriented LLMs.",
                            "citation_reason": "Extends persona conditioning to recommendation scenarios, diversifying application domains."
                        }
                    ]
                },
                {
                    "text": "Continual personalisation techniques tackle catastrophic forgetting while adapting to evolving user signals.",
                    "papers": [
                        {
                            "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2407.00693",
                            "summary": "Proposes anchoring to the base model's initial responses to retain global knowledge during personalised preference optimisation.",
                            "citation_reason": "Adds a lightweight yet effective mitigation strategy against forgetting."
                        },
                        {
                            "title": "Personalized Large Language Model Assistant with Evolving Conditional Memory",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2312.17257",
                            "summary": "Introduces conditional memory that stores and retrieves user-specific dialogue snippets to improve future responses.",
                            "citation_reason": "Illustrates how memory mechanisms enable long-term adaptation beyond static fine-tuning."
                        }
                    ]
                }
            ]
        },
        {
            "number": 3,
            "title": "Synthesis and Critical Discussion",
            "outline": "Compare empirical findings, scrutinise methodological robustness, and surface contradictions or open problems.",
            "key_points": [
                {
                    "text": "Evaluation remains fragmented; emerging benchmarks expose substantial gaps in measuring fidelity to user personas.",
                    "papers": [
                        {
                            "title": "RecExplainer: Aligning Large Language Models for Explaining Recommendation Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2311.10947",
                            "summary": "Trains LLM surrogates to mimic recommendation models and generate user-customised explanations, revealing evaluation challenges.",
                            "citation_reason": "Provides practical evidence of evaluation difficulties in domain-specific personalisation."
                        },
                        {
                            "title": "PersonaGym: Evaluating Persona Agents and LLMs",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2407.18416",
                            "summary": "Offers a scalable test bed and automatic metric for persona fidelity, showing that model size does not guarantee consistency.",
                            "citation_reason": "Reused to highlight its central role in addressing the evaluation deficit across studies."
                        }
                    ]
                },
                {
                    "text": "Studies report a tension between personalisation gains and knowledge retention, underscoring the risk of catastrophic forgetting.",
                    "papers": [
                        {
                            "title": "Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2310.18628",
                            "summary": "Implements a student-driven distillation loop that selectively learns only from mistaken cases, improving efficiency but risking scope loss.",
                            "citation_reason": "Highlights selective fine-tuning as both a gain and a potential source of forgetting."
                        },
                        {
                            "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2407.00693",
                            "summary": "Anchors learning to the base model\u2019s initial outputs to reduce knowledge erosion during personalised optimisation.",
                            "citation_reason": "Reused because it directly addresses the forgetting problem contrasted in this point."
                        }
                    ]
                },
                {
                    "text": "Fairness and diversity concerns surface when aggregate models inadequately capture minority preferences or client disparities.",
                    "papers": [
                        {
                            "title": "Can Fair Federated Learning reduce the need for Personalisation?",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2305.02728",
                            "summary": "Shows that fairness-oriented FL algorithms do not always improve under-performing clients and proposes personalisation-aware FL.",
                            "citation_reason": "Provides empirical evidence that aggregate strategies may still fail minority users."
                        },
                        {
                            "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.16455",
                            "summary": "Demonstrates preference collapse under KL regularisation, particularly harming minority preference distributions.",
                            "citation_reason": "Reused to emphasise systemic bias risks corroborated by multiple studies."
                        }
                    ]
                }
            ]
        },
        {
            "number": 4,
            "title": "Conclusion: Current State of Knowledge",
            "outline": "Summarise converging insights, highlighting what is now well-understood in personalised and conditional alignment.",
            "key_points": [
                {
                    "text": "Hybrid and context-aware RLHF methods are converging on sample-efficient, user-specific optimisation while mitigating distributional shifts.",
                    "papers": [
                        {
                            "title": "Low-Rank Contextual Reinforcement Learning from Heterogeneous Human Feedback",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2412.19436",
                            "summary": "Uses low-rank factorisation to handle high-dimensional user contexts, achieving robust alignment across feedback heterogeneity.",
                            "citation_reason": "Shows maturity in scalable contextual RLHF techniques central to the field's progress."
                        },
                        {
                            "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.18571",
                            "summary": "Demonstrates intuitive vector-arithmetic control over alignment trade-offs, signalling practical usability advances.",
                            "citation_reason": "Reused to capture consensus on controllable multi-objective optimisation."
                        }
                    ]
                },
                {
                    "text": "Persona conditioning and data-centric approaches have matured to provide stable, multi-facet personality control in open-domain dialogue.",
                    "papers": [
                        {
                            "title": "MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2310.18342",
                            "summary": "Achieves flexible attribute composition via latent energy-based modelling, reducing reliance on textual personas.",
                            "citation_reason": "Encapsulates technical maturity in latent persona control."
                        },
                        {
                            "title": "Dialogue Language Model with Large-Scale Persona Data Engineering",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2412.09034",
                            "summary": "Shows that large-scale automatically extracted persona data markedly improves persona consistency.",
                            "citation_reason": "Demonstrates data-driven consolidation of persona consistency techniques."
                        }
                    ]
                }
            ]
        },
        {
            "number": 5,
            "title": "Future Directions and Open Challenges",
            "outline": "Identify unresolved issues, methodological gaps, and promising avenues for further research.",
            "key_points": [
                {
                    "text": "Integrating safety constraints with personalised alignment requires new techniques to bound harmful content while respecting diverse preferences.",
                    "papers": [
                        {
                            "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2303.05453",
                            "summary": "Proposes policy tiers to restrict unsafe behaviours without eliminating user-desired personalisation.",
                            "citation_reason": "Reused to emphasise the need for policy-tech co-design in future work."
                        },
                        {
                            "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.16455",
                            "summary": "Highlights systemic biases that can arise during preference alignment and proposes regularisation remedies.",
                            "citation_reason": "Reused to motivate safety-aware algorithmic research on bias mitigation."
                        }
                    ]
                },
                {
                    "text": "Real-time adaptive memory and user-profiling mechanisms should move toward privacy-preserving, on-device inference for continual personalisation.",
                    "papers": [
                        {
                            "title": "Personalized Large Language Model Assistant with Evolving Conditional Memory",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2312.17257",
                            "summary": "Introduces conditional memory that incrementally updates with dialogue history to tailor future responses.",
                            "citation_reason": "Points to online user-state tracking as a frontier for responsive personalisation."
                        },
                        {
                            "title": "ULMRec: User-centric Large Language Model for Sequential Recommendation",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2412.05543",
                            "summary": "Uses vector-quantised user representations and alignment tuning, hinting at efficient on-device personalisation for recommender systems.",
                            "citation_reason": "Shows the feasibility and challenges of embedding rich user profiles in live systems."
                        }
                    ]
                },
                {
                    "text": "Community-accepted benchmarks and unified metrics are essential to compare methods fairly and accelerate reproducible progress.",
                    "papers": [
                        {
                            "title": "PersonaGym: Evaluating Persona Agents and LLMs",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2407.18416",
                            "summary": "Provides a large-scale, multi-environment testbed and human-aligned automatic metric for persona adherence.",
                            "citation_reason": "Reused as a blueprint for future standardised benchmarking efforts."
                        },
                        {
                            "title": "RecExplainer: Aligning Large Language Models for Explaining Recommendation Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2311.10947",
                            "summary": "Demonstrates task-specific evaluation challenges and the need for domain-tailored metrics.",
                            "citation_reason": "Reused to stress that evaluation must extend beyond dialogue into vertical applications."
                        }
                    ]
                }
            ]
        }
    ]
}