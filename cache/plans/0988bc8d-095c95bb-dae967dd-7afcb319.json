{
    "reasoning": "To build a coherent graduate-level literature review, I first mapped the topic into six logically-ordered sections that follow the requested template (background, taxonomy, methods, evaluation, applications, and future directions). I next collected 24 peer-reviewed or widely-cited arXiv papers (≥ 2024) using the provided refined search queries. Each paper was categorised by its primary contribution (e.g., large-scale preference datasets, on-device personalisation, multi-objective alignment, evaluation benchmarks, domain applications, or emerging challenges). \n\nTo maximise coverage while satisfying the non-duplication constraint, each paper is cited in only one section. I assigned two key points per section, each supported by exactly two unique papers (12 key points × 2 papers = 24 citations). This evenly distributes the literature, prevents redundancy, and showcases both technical depth (e.g., MetaAligner, Rewards-in-Context) and practical advances (e.g., TAPS, Personalized Web Agents). Central but orthogonal works (e.g., NAACL 2025 tutorial) are placed in the background to anchor definitions without being reused elsewhere.\n\nThe result is a comprehensive yet concise outline that logically flows from motivation through taxonomy and methods, into evaluation, real-world implications, and open challenges, while giving balanced representation to all collected papers.",
    "plan": [
        {
            "number": 1,
            "title": "Background and Motivation",
            "outline": "Introduces why aligning LLM behaviour to individual users or conditional objectives matters, framing the economic, technical and ethical stakes that motivate recent research.",
            "key_points": [
                {
                    "text": "Scaling personalised alignment: recent work shows the need to move beyond one-size-fits-all alignment toward modelling millions of distinct user preferences.",
                    "papers": [
                        {
                            "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2503.15463",
                            "summary": "Introduces AlignX, a 1.3 M-example dataset and two alignment strategies that significantly improve user-level preference following.",
                            "citation_reason": "Demonstrates the real-world scale and necessity of personalised alignment."
                        },
                        {
                            "title": "NAACL 2025 Tutorial: Adaptation of Large Language Models",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2504.03931",
                            "summary": "Survey tutorial outlining the spectrum of LLM adaptation techniques, highlighting personalisation as a core industrial demand.",
                            "citation_reason": "Provides an authoritative overview that contextualises the problem space."
                        }
                    ]
                },
                {
                    "text": "Resource-aware personalisation: mobile, privacy-preserving, and federated settings push for lightweight yet effective adaptation.",
                    "papers": [
                        {
                            "title": "PLMM: Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Proposes distilled personal LLMs that run on-device while encrypting user data, outlining a three-level model hierarchy.",
                            "citation_reason": "Highlights computational and privacy motivations behind personal LLMs."
                        },
                        {
                            "title": "Personalized Federated Instruction Tuning via Neural Architecture Search",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.16919",
                            "summary": "Introduces PerFIT, allowing each client to search a bespoke adapter architecture during federated instruction tuning, reducing perplexity by up to 23%.",
                            "citation_reason": "Shows federated personalisation as a practical response to privacy/resource constraints."
                        }
                    ]
                }
            ]
        },
        {
            "number": 2,
            "title": "Taxonomy of Personalisation and Conditional Alignment",
            "outline": "Organises the literature into meaningful categories by granularity, conditioning signal, and adaptation mode, providing a conceptual map for the review.",
            "key_points": [
                {
                    "text": "Persona-level versus individual-specific modelling: contrasting role-playing agents with fine-grained personality generation.",
                    "papers": [
                        {
                            "title": "Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2411.10006",
                            "summary": "Presents Orca, a framework that injects Big-Five personality signals into LLMs and releases OrcaBench for evaluation.",
                            "citation_reason": "Represents persona-conditioned alignment within dialogue agents."
                        },
                        {
                            "title": "Dynamic Generation of Personalities with Large Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2404.07084",
                            "summary": "Introduces Hypernetwork-based Dynamic Personality Generation and a new metric for personality realism.",
                            "citation_reason": "Illustrates individual-specific personality conditioning, complementing Orca."
                        }
                    ]
                },
                {
                    "text": "Prompt-level conditioning versus demonstration-level adaptation: leveraging in-context signals for on-the-fly personalisation.",
                    "papers": [
                        {
                            "title": "Conversational Prompt Engineering",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2408.04560",
                            "summary": "Proposes CPE, a chat-based system to interactively craft personalised prompts that rival few-shot quality.",
                            "citation_reason": "Embodies explicit prompt conditioning within the taxonomy."
                        },
                        {
                            "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2506.17930",
                            "summary": "Introduces PromptQuine, an evolutionary search that prunes demonstrations into ‘gibberish’ yet highly effective prompts.",
                            "citation_reason": "Represents automatic, demonstration-level optimisation for conditional alignment."
                        }
                    ]
                }
            ]
        },
        {
            "number": 3,
            "title": "Methods and Algorithmic Approaches",
            "outline": "Surveys the core technical strategies used to achieve personalisation or conditional alignment, comparing parameter efficiency, optimisation paradigms, and objective design.",
            "key_points": [
                {
                    "text": "Parameter-efficient and on-device adaptation: lightweight models and communication-aware meta-learning.",
                    "papers": [
                        {
                            "title": "Goldfish: Monolingual Language Models for 350 Languages",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2408.10441",
                            "summary": "Releases 125 M-parameter monolingual LMs for 350 languages that outperform larger multilingual baselines on perplexity.",
                            "citation_reason": "Demonstrates small, specialised models enabling device-level personalisation."
                        },
                        {
                            "title": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2406.11569",
                            "summary": "Studies federated meta-learning with over-the-air aggregation, analysing the trade-off between convergence and generalisation in personalised LMs.",
                            "citation_reason": "Provides a meta-learning perspective on efficient personal adaptation."
                        }
                    ]
                },
                {
                    "text": "Multi-objective and preference-based alignment: optimising LLMs to satisfy heterogeneous or conflicting user values.",
                    "papers": [
                        {
                            "title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2403.17141",
                            "summary": "Proposes a policy-agnostic three-stage aligner that achieves balanced gains across objectives while cutting GPU hours by up to 93%.",
                            "citation_reason": "State-of-the-art algorithm explicitly targeting conditional, multi-objective alignment."
                        },
                        {
                            "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.10207",
                            "summary": "Introduces RiC, a supervised fine-tuning scheme that conditions generation on reward descriptions, enabling dynamic preference control at inference.",
                            "citation_reason": "Showcases an alternative conditioning mechanism using reward prompts."
                        }
                    ]
                }
            ]
        },
        {
            "number": 4,
            "title": "Evaluation and Benchmarks",
            "outline": "Details how personalisation and conditional alignment are measured, covering bespoke datasets, behavioural metrics, and analysis toolkits.",
            "key_points": [
                {
                    "text": "Task-oriented personalisation benchmarks: measuring preference adherence in dialogue and web interaction.",
                    "papers": [
                        {
                            "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2506.20409",
                            "summary": "Introduces a structured tagging tool and uncertainty-based detector that set SOTA on the NLSI personalised dialogue task.",
                            "citation_reason": "Provides an open benchmark for evaluating personalised tool use."
                        },
                        {
                            "title": "Large Language Models Empowered Personalized Web Agents",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2410.17236",
                            "summary": "Releases PersonalWAB, a benchmark with personalised instructions, user data and web functions, plus the PUMA alignment framework.",
                            "citation_reason": "Adds a multi-task evaluation suite for user-specific web agents."
                        }
                    ]
                },
                {
                    "text": "Analytical probes of alignment dynamics: probing when and how contextual alignment emerges inside models.",
                    "papers": [
                        {
                            "title": "Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2411.00646",
                            "summary": "Reveals a four-phase contextualisation pattern in VLLMs, distinguishing alignment, encoding, fusion and output stages.",
                            "citation_reason": "Offers methodology to dissect alignment stages, informing evaluation design."
                        },
                        {
                            "title": "Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.15109",
                            "summary": "Shows that tokenisation and multi-head attention drive brain-aligned representations even in untrained networks.",
                            "citation_reason": "Connects alignment metrics to cognitive plausibility, enriching evaluation angles."
                        }
                    ]
                }
            ]
        },
        {
            "number": 5,
            "title": "Applications and Societal Impact",
            "outline": "Explores how personalised or conditionally aligned LLMs are deployed in practice and how they address (or introduce) ethical and operational concerns.",
            "key_points": [
                {
                    "text": "Bias detection and mitigation through personalised context: leveraging user conditioning to improve or audit safety.",
                    "papers": [
                        {
                            "title": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2505.02252",
                            "summary": "Investigates how country-specific personas alter hate-speech judgements and introduces debias tuning to correct inconsistencies.",
                            "citation_reason": "Demonstrates real-world safety stakes of personalised context."
                        },
                        {
                            "title": "T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.16146",
                            "summary": "Proposes a tree-based method for generating counterfactual explanations that remain valid across changing ML models.",
                            "citation_reason": "Shows how preference-aware explanations can support transparency."
                        }
                    ]
                },
                {
                    "text": "Domain-specific personalised assistants: education, databases and beyond.",
                    "papers": [
                        {
                            "title": "Educational Personalized Learning Path Planning with Large Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2407.11773",
                            "summary": "Uses prompt engineering with LLama-2-70B and GPT-4 to design learner-tailored study paths, showing gains in user satisfaction.",
                            "citation_reason": "Illustrates personalised alignment in the educational domain."
                        },
                        {
                            "title": "Trustworthy and Efficient LLMs Meet Databases",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2412.18022",
                            "summary": "Tutorial explores integrating LLMs with database systems for more trustworthy and efficient query answering.",
                            "citation_reason": "Highlights emerging vertical applications that rely on conditional alignment."
                        }
                    ]
                }
            ]
        },
        {
            "number": 6,
            "title": "Open Challenges and Future Directions",
            "outline": "Identifies persisting gaps—such as cross-lingual generalisation, calibration, and adaptive preference discovery—and proposes promising research avenues.",
            "key_points": [
                {
                    "text": "Cross-lingual personalisation: aligning LLMs to diverse cultural and linguistic contexts remains under-explored.",
                    "papers": [
                        {
                            "title": "Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual Performance of LLMs",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2504.09378",
                            "summary": "Shows strong language-level but weak instance-level correlation between alignment metrics and task success, questioning adequacy of current techniques.",
                            "citation_reason": "Highlights the difficulty of extending conditional alignment across languages."
                        },
                        {
                            "title": "Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2504.04152",
                            "summary": "Systematically evaluates 36 CPT configurations, revealing nuanced interactions between language pairs and code data.",
                            "citation_reason": "Points to open questions about optimal data strategies for multilingual personalisation."
                        }
                    ]
                },
                {
                    "text": "Dynamic preference discovery and calibration: keeping models aligned as user goals evolve while maintaining reliable confidence.",
                    "papers": [
                        {
                            "title": "ADAPT: Actively Discovering and Adapting to Preferences for any Task",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2504.04040",
                            "summary": "Introduces a benchmark and Reflection-DPO for agents that actively question users to satisfy preferences, improving success by 6.1%.",
                            "citation_reason": "Frames the challenge of on-line preference elicitation and adaptation."
                        },
                        {
                            "title": "Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2505.20903",
                            "summary": "Proposes CogCalib, a cognition-aware fine-tuning strategy that cuts expected calibration error by 57%.",
                            "citation_reason": "Underscores the necessity of maintaining calibrated confidence during personalised alignment."
                        }
                    ]
                }
            ]
        }
    ]
}