{
    "reasoning": "I first enumerated recent (2023-2025) arXiv papers returned by the refined topic searches and filtered for those that are either peer-reviewed elsewhere or already widely cited. Fourteen unique papers covered all critical angles: mobile/edge personalization, personality alignment, RLHF bias, proxy-based alignment, synthetic preference datasets, demographic fairness benchmarks, and forgetting-aware optimization. To minimise duplication while ensuring coverage, I mapped these papers to five logical sections that mirror the classic flow of a graduate literature review. Each section contains two key points (for conciseness) and each point cites two papers, giving 20 total citations. Because the paper set (14) is smaller than the citation slots (20), six strategically chosen papers are reused exactly once in later sections where they are genuinely central; the citation_reason field in those repeat appearances explicitly justifies the reuse. This structure guarantees: (1) no section reuses a paper already used there, (2) every retrieved paper appears at least once, (3) even distribution of citations, and (4) a clear progression from motivation \u2192 taxonomy \u2192 methods \u2192 evaluation \u2192 future directions.",
    "plan": [
        {
            "number": 1,
            "title": "Background and Motivation",
            "outline": "Introduces why aligning LLMs to user attributes (personality, demographics, device constraints) is crucial and frames societal fairness and RLHF bias as driving forces behind current research.",
            "key_points": [
                {
                    "text": "The emergence of edge and mobile scenarios highlights the need for lightweight personalised LLMs that can incorporate individual traits without cloud dependence.",
                    "papers": [
                        {
                            "title": "PLMM: Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Proposes a hierarchy of personal, expert and traditional LLMs and presents distillation techniques to run personal models efficiently on mobile hardware while protecting user privacy.",
                            "citation_reason": "Demonstrates real-world necessity and feasibility of on-device personalised LLMs, motivating the review."
                        },
                        {
                            "title": "Personality Alignment of Large Language Models",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2408.11779",
                            "summary": "Introduces the concept of personality alignment, a large PAPI dataset of 320k humans, and an activation-intervention optimisation method that quickly adapts models to personality traits.",
                            "citation_reason": "Provides concrete evidence that user-specific personality alignment is becoming an explicit research goal."
                        }
                    ]
                },
                {
                    "text": "Fairness across demographic groups in sensitive domains such as healthcare motivates conditional alignment beyond one-size-fits-all optimisation.",
                    "papers": [
                        {
                            "title": "FMBench: Benchmarking Fairness in Multimodal Large Language Models on Medical Tasks",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2410.01089",
                            "summary": "Presents the first multimodal medical fairness benchmark covering race, ethnicity, language and gender with both lexical and clinically informed metrics.",
                            "citation_reason": "Illustrates real-world stakes of demographic alignment in a high-risk application domain."
                        },
                        {
                            "title": "Robustness and Confounders in the Demographic Alignment of LLMs with Human Perceptions of Offensiveness",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2411.08977",
                            "summary": "Analyses five offensive-language datasets and shows that apparent demographic bias is often entangled with task difficulty and annotator sensitivity, urging more nuanced alignment measures.",
                            "citation_reason": "Highlights methodological challenges that underpin the motivation for conditional alignment research."
                        }
                    ]
                }
            ]
        },
        {
            "number": 2,
            "title": "Taxonomy of Personalisation and Conditional Alignment Techniques",
            "outline": "Organises the literature into coherent categories\u2014persona-controlled generation, synthetic preference datasets, bias-aware RLHF, and proxy-based alignment\u2014setting the stage for deeper methodological comparisons.",
            "key_points": [
                {
                    "text": "Persona modelling and synthetic preference data enable fine-grained control over multiple user attributes during generation.",
                    "papers": [
                        {
                            "title": "MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2310.18342",
                            "summary": "Disentangles complex personalities into multiple latent attributes and applies an energy-based CVAE for controllable personalised dialogue.",
                            "citation_reason": "Represents the latent-attribute control category within the taxonomy."
                        },
                        {
                            "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2505.11861",
                            "summary": "Generates 238k synthetic preference records across 28 social groups and 98 equity topics, enabling alignment to specific equity-oriented personas.",
                            "citation_reason": "Illustrates the synthetic-dataset approach to conditional preference alignment."
                        }
                    ]
                },
                {
                    "text": "Algorithm-level categories include bias-aware RLHF regularisation and proxy models that decouple generation from alignment.",
                    "papers": [
                        {
                            "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.16455",
                            "summary": "Shows how KL-based RLHF can cause minority preference collapse and proposes preference-matching regularisation to mitigate the bias.",
                            "citation_reason": "Defines the bias-aware RLHF subgroup within the taxonomy."
                        },
                        {
                            "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2403.04283",
                            "summary": "Introduces an RLHF variant that trains a lightweight proxy to supervise token generation, reducing compute by 99 % while maintaining alignment.",
                            "citation_reason": "Represents decoupled proxy-based alignment methods in the taxonomy."
                        }
                    ]
                }
            ]
        },
        {
            "number": 3,
            "title": "Methods and Algorithms",
            "outline": "Analyses state-of-the-art optimisation pipelines\u2014practical RLHF systems, curiosity-driven reinforcement, and forgetting-aware preference optimisation\u2014highlighting their assumptions, computational costs and adaptation capabilities.",
            "key_points": [
                {
                    "text": "Practical reinforcement frameworks extend RLHF with scalable pipelines or intrinsic rewards to enhance personalisation in multi-turn settings.",
                    "papers": [
                        {
                            "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2404.00934",
                            "summary": "Details a production RLHF system with reward variance mitigation, parallel training and regularisation that improves ChatGLM alignment by 15 %.",
                            "citation_reason": "Serves as a real-world baseline for large-scale RLHF personalisation pipelines."
                        },
                        {
                            "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2504.03206",
                            "summary": "Introduces a curiosity-based intrinsic reward that encourages the agent to infer user traits during dialogue, boosting personalisation without long histories.",
                            "citation_reason": "Demonstrates reinforcement signals specifically designed to elicit user attributes."
                        }
                    ]
                },
                {
                    "text": "Preference-optimisation methods tackle catastrophic forgetting and heterogeneous user goals in sequential fine-tuning scenarios.",
                    "papers": [
                        {
                            "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2407.00693",
                            "summary": "Anchors optimisation to the reference model\u2019s initial responses, enabling alignment to diverse preferences while retaining global knowledge.",
                            "citation_reason": "Presents a forgetting-resilient optimisation algorithm central to personalised fine-tuning."
                        },
                        {
                            "title": "LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2404.14285",
                            "summary": "Combines imitation learning and iterative self-training to align an LLM planner with household user preferences, achieving >30 % higher task success.",
                            "citation_reason": "Extends preference optimisation to embodied planning tasks, showcasing cross-domain applicability."
                        }
                    ]
                }
            ]
        },
        {
            "number": 4,
            "title": "Evaluation and Benchmarks",
            "outline": "Surveys emerging datasets, metrics and theoretical frameworks for measuring alignment quality, fairness and robustness across user subgroups and tasks.",
            "key_points": [
                {
                    "text": "Resource libraries and recommender-system testbeds provide systematic ways to quantify preference alignment and fairness trade-offs.",
                    "papers": [
                        {
                            "title": "SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2412.16783",
                            "summary": "Offers an open library that standardises diverse datasets for consistent evaluation of perspective-aligned LLMs across tasks like hate-speech detection.",
                            "citation_reason": "Supplies a theory-driven, multi-construct framework essential for benchmark unification."
                        },
                        {
                            "title": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2403.05668",
                            "summary": "Proposes an evaluation protocol that factors true preference alignment and intersectional sensitive attributes in recommender scenarios.",
                            "citation_reason": "Introduces task-specific metrics that align fairness with authentic user tastes."
                        }
                    ]
                },
                {
                    "text": "Multi-modal and text-only benchmarks measure demographic parity and surface confounders, guiding future alignment work.",
                    "papers": [
                        {
                            "title": "FMBench: Benchmarking Fairness in Multimodal Large Language Models on Medical Tasks",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2410.01089",
                            "summary": "Provides free-form VQA and report-generation tasks with race, ethnicity and gender labels, plus the Fairness-Aware Performance metric.",
                            "citation_reason": "Reused here to exemplify comprehensive, clinically grounded evaluation of demographic alignment."
                        },
                        {
                            "title": "Robustness and Confounders in the Demographic Alignment of LLMs with Human Perceptions of Offensiveness",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2411.08977",
                            "summary": "Shows that within-group agreement and document difficulty often outweigh pure demographic factors in alignment scores.",
                            "citation_reason": "Reused because it uniquely uncovers hidden confounders, informing benchmark design."
                        }
                    ]
                }
            ]
        },
        {
            "number": 5,
            "title": "Open Challenges and Future Directions",
            "outline": "Discusses unresolved issues such as privacy-preserving on-device models, catastrophic forgetting, data-quality trade-offs and the ethical design of synthetic preferences.",
            "key_points": [
                {
                    "text": "Scaling personalisation while preserving privacy and avoiding knowledge erosion remains an open problem.",
                    "papers": [
                        {
                            "title": "PLMM: Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Advocates for small, privacy-preserving on-device LLMs, outlining latency and resource constraints.",
                            "citation_reason": "Reused to highlight scalability and privacy as persistent future challenges."
                        },
                        {
                            "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2407.00693",
                            "summary": "Demonstrates that preference heterogeneity can exacerbate forgetting, motivating new research into continual alignment methods.",
                            "citation_reason": "Reused because it directly targets catastrophic forgetting, a key open issue."
                        }
                    ]
                },
                {
                    "text": "Synthesising high-quality, ethically grounded preference data and balancing individual versus group alignment warrant further investigation.",
                    "papers": [
                        {
                            "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2505.11861",
                            "summary": "Generates a large synthetic dataset rooted in social-survey evidence and proposes re-weighting to maximise divergence between personas.",
                            "citation_reason": "Reused to emphasise both opportunities and ethical complexities of synthetic preference data."
                        },
                        {
                            "title": "Personality Alignment of Large Language Models",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2408.11779",
                            "summary": "Shows efficient activation-intervention optimisation, but raises questions about harmful trait alignment and limited personal data.",
                            "citation_reason": "Reused to underscore the tension between fine-grained personalisation and safe, equitable alignment."
                        }
                    ]
                }
            ]
        }
    ]
}