{
    "reasoning": "I first compiled 16 post-2023 arXiv or peer-reviewed papers that explicitly address either (i) user-specific personalisation of large language models or (ii) conditional/value alignment techniques that adapt generation to heterogeneous preferences.  Papers were clustered into four methodological macro-themes: on-device or federated personalisation, persona / user-conditioning, multi-objective reward modelling, and safety-oriented value alignment.  These four themes naturally map onto the \u2018Thematic Landscape\u2019 section.  The Introduction draws on two seminal personalisation papers to motivate the topic and two alignment papers to illustrate emerging risks, without overlap with the Landscape citations.  The Synthesis section then re-uses a subset of papers (explicitly justified) to compare methods, interrogate trade-offs and expose evaluation shortcomings.  Conclusion and Future Directions selectively repeat critical exemplars to highlight consensus and outline research gaps.  All 16 papers are cited at least once; duplication is limited to works that are foundational for cross-section discussion (justified via \"citation_reason\").  Citations are evenly distributed\u2014each paper appears one to three times, ensuring breadth while maintaining narrative flow.",
    "plan": [
        {
            "number": 1,
            "title": "Introduction: Personalisation and Conditional Alignment of LLMs",
            "outline": "Define personalisation and conditional alignment, explain their significance for user trust, safety, and utility, and delineate the review\u2019s scope (post-2023 methodological advances).",
            "key_points": [
                {
                    "text": "Growing demand for LLMs that adapt to individual users\u2019 contexts, devices, and preferences underscores the need for lightweight yet effective personalisation mechanisms.",
                    "papers": [
                        {
                            "title": "PLMM: Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Introduces a three-tier architecture that distils large models into privacy-preserving, real-time personal LLMs deployable on mobile hardware.",
                            "citation_reason": "Provides a clear motivation for on-device, user-specific models, anchoring the personalisation problem."
                        },
                        {
                            "title": "Factual and Personalized Recommendations using Language Models and Reinforcement Learning",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2310.06176",
                            "summary": "Proposes P4LM, which combines user-preference embeddings with RL-based reward optimisation to produce compelling, fact-grounded personalised recommendations.",
                            "citation_reason": "Illustrates commercial relevance and early success of RL-enhanced personalisation."
                        }
                    ]
                },
                {
                    "text": "Conditional alignment has emerged to mitigate hallucination, bias, and domain mis-grounding by explicitly linking model behaviour to user or task constraints.",
                    "papers": [
                        {
                            "title": "The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2305.13669",
                            "summary": "Formulates knowledge alignment and proposes MixAlign, a framework that interacts with users and knowledge bases to reduce hallucinations by up to 22%.",
                            "citation_reason": "Demonstrates why conditional alignment is essential for faithfulness and safety."
                        },
                        {
                            "title": "Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.04481",
                            "summary": "Extends RLHF with multi-agent human feedback to align an autonomous-driving LLM toward safety-critical objectives.",
                            "citation_reason": "Shows domain-specific stakes of alignment, broadening the introductory context."
                        }
                    ]
                }
            ]
        },
        {
            "number": 2,
            "title": "Methodological Landscape: Contemporary Approaches",
            "outline": "Map recent work into four methodological clusters\u2014federated/on-device personalisation, persona-driven modelling, multi-objective reward optimisation, and safety-centric value alignment\u2014highlighting distinctive techniques and evaluation paradigms.",
            "key_points": [
                {
                    "text": "Federated and on-device techniques balance personalisation, generalisation, and privacy through prompt-level or adapter-level updates.",
                    "papers": [
                        {
                            "title": "Harmonizing Generalization and Personalization in Federated Prompt Learning",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.09771",
                            "summary": "FedPGP couples CLIP guidance with low-rank prompt adaptations to trade off global generalisation and client-specific tailoring.",
                            "citation_reason": "Representative of prompt-level federated personalisation strategies."
                        },
                        {
                            "title": "Dual-Personalizing Adapter for Federated Foundation Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2403.19211",
                            "summary": "Introduces FedDPA, which integrates global and local adapters with dynamic weighting to handle test-time distribution shifts.",
                            "citation_reason": "Shows adapter-based dual-personalisation under realistic non-IID settings."
                        },
                        {
                            "title": "Now It Sounds Like You: Learning Personalized Vocabulary On Device",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2305.03584",
                            "summary": "Presents an OOV adapter that learns user-specific word embeddings under federated constraints, improving accuracy without extra latency.",
                            "citation_reason": "Illustrates privacy-preserving lexical personalisation at the edge."
                        }
                    ]
                },
                {
                    "text": "Persona-oriented modelling embeds explicit or inferred user traits to steer dialogue style, consistency, and content.",
                    "papers": [
                        {
                            "title": "Dynamic Generation of Personalities with Large Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2404.07084",
                            "summary": "DPG fine-tunes hypernetworks on a personality-annotated corpus, enhancing controllable persona generation.",
                            "citation_reason": "Represents model-side generation of dynamic persona embeddings."
                        },
                        {
                            "title": "Quantifying the Persona Effect in LLM Simulations",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.10811",
                            "summary": "Measures how demographic and behavioural persona prompts affect LLM predictions across subjective tasks.",
                            "citation_reason": "Provides empirical evidence on the efficacy and limits of persona prompting."
                        },
                        {
                            "title": "Dialogue Language Model with Large-Scale Persona Data Engineering",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2412.09034",
                            "summary": "PPDS constructs massive persona-dialogue data via automatic extraction and augmentation, boosting persona consistency.",
                            "citation_reason": "Represents data-centric scaling strategies for persona consistency."
                        }
                    ]
                },
                {
                    "text": "Multi-objective reward modelling extends RLHF to capture heterogeneous user preferences and enable nuanced trade-offs.",
                    "papers": [
                        {
                            "title": "Multi-objective Reinforcement Learning from AI Feedback",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.07295",
                            "summary": "MORLAIF decomposes rewards into principle-specific models (toxicity, factuality, etc.) and scalarises them for PPO training.",
                            "citation_reason": "Shows explicit multi-objective decomposition as an alignment strategy."
                        },
                        {
                            "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.18571",
                            "summary": "DPA models user preferences as unit-vector directions in reward space, enabling intuitive arithmetic control (e.g., helpfulness vs. verbosity).",
                            "citation_reason": "Demonstrates user-conditioned control via geometric reward representations."
                        },
                        {
                            "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.12845",
                            "summary": "ArmoRM trains an absolute-rating, objective-specific reward model and a MoE gating network for context-aware objective selection.",
                            "citation_reason": "Introduces interpretability into multi-objective reward frameworks."
                        }
                    ]
                },
                {
                    "text": "Safety-centric value alignment research examines how users identify, challenge, and reshape model behaviours perceived as harmful.",
                    "papers": [
                        {
                            "title": "User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2409.00862",
                            "summary": "Qualitative study of 77 posts and 20 interviews revealing seven user strategies for correcting discriminatory outputs.",
                            "citation_reason": "Highlights end-user agency in practical value alignment."
                        },
                        {
                            "title": "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2409.19723",
                            "summary": "Introduces PersonalityEvd dataset and a chain-of-evidence framework for explainable personality inference.",
                            "citation_reason": "Provides an evaluation resource linking personality modelling to transparent alignment."
                        },
                        {
                            "title": "Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2407.02099",
                            "summary": "Systematically studies 162 personas across seven LLMs, revealing variability in objective and subjective tasks.",
                            "citation_reason": "Connects persona design to safety-critical behaviour changes."
                        }
                    ]
                }
            ]
        },
        {
            "number": 3,
            "title": "Synthesis & Critical Discussion",
            "outline": "Compare approaches, identify converging findings, expose contradictions, and evaluate methodological robustness across studies.",
            "key_points": [
                {
                    "text": "A persistent trade-off emerges between client-level personalisation and global generalisation, with adapter and prompt strategies offering complementary solutions.",
                    "papers": [
                        {
                            "title": "Harmonizing Generalization and Personalization in Federated Prompt Learning",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2405.09771",
                            "summary": "Balances CLIP-guided global prompts with low-rank local adaptations.",
                            "citation_reason": "Empirically quantifies the generalisation-personalisation tension."
                        },
                        {
                            "title": "Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Distils compact personal models that prioritise local context over universality.",
                            "citation_reason": "Contrasts with FedPGP by emphasising extreme on-device personalisation."
                        }
                    ]
                },
                {
                    "text": "Multi-objective RLHF frameworks increase alignment flexibility but raise concerns about reward interpretability and optimisation stability.",
                    "papers": [
                        {
                            "title": "Multi-objective Reinforcement Learning from AI Feedback",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.07295",
                            "summary": "Shows improved alignment but notes negligible differences across scalarisation functions.",
                            "citation_reason": "Provides empirical evidence of optimisation instability sources."
                        },
                        {
                            "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.12845",
                            "summary": "Introduces MoE gating to make objective weighting transparent.",
                            "citation_reason": "Addresses interpretability gap highlighted by MORLAIF."
                        },
                        {
                            "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.18571",
                            "summary": "Offers geometry-based user control, but relies on accurate reward vectors.",
                            "citation_reason": "Illustrates alternative design space and its limits."
                        }
                    ]
                },
                {
                    "text": "Evaluating persona and value alignment remains inconsistent, with emerging datasets and human-in-the-loop studies pointing to scalability and bias challenges.",
                    "papers": [
                        {
                            "title": "Revealing Personality Traits: PersonalityEvd Dataset",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2409.19723",
                            "summary": "Offers chain-of-evidence annotation for explainability.",
                            "citation_reason": "Supplies much-needed benchmark for systematic evaluation."
                        },
                        {
                            "title": "Quantifying the Persona Effect in LLM Simulations",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.10811",
                            "summary": "Finds persona variables explain <10% variance in subjective datasets.",
                            "citation_reason": "Highlights weak signal and evaluation noise in persona prompts."
                        },
                        {
                            "title": "User-Driven Value Alignment",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2409.00862",
                            "summary": "Documents real users\u2019 ad-hoc alignment tactics.",
                            "citation_reason": "Reveals ecological validity gap between lab metrics and real-world alignment."
                        }
                    ]
                }
            ]
        },
        {
            "number": 4,
            "title": "Conclusion",
            "outline": "Summarise convergent insights, delineate the current state of personalised and conditionally aligned LLM research, and articulate overall lessons.",
            "key_points": [
                {
                    "text": "Research is coalescing around hybrid architectures (global + local adapters) and multi-objective reward models that jointly pursue personalisation and alignment.",
                    "papers": [
                        {
                            "title": "Dual-Personalizing Adapter for Federated Foundation Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2403.19211",
                            "summary": "Demonstrates dynamic weighting between global and local adapters for robust test-time shifts.",
                            "citation_reason": "Embodies the hybrid architectural trend."
                        },
                        {
                            "title": "Multi-objective Reinforcement Learning from AI Feedback",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.07295",
                            "summary": "Validates the feasibility of decomposed reward spaces.",
                            "citation_reason": "Represents convergence toward multi-objective alignment."
                        }
                    ]
                },
                {
                    "text": "Standardised evaluation resources and interactive alignment mechanisms are still nascent, limiting cross-study comparability and real-world reliability.",
                    "papers": [
                        {
                            "title": "The Knowledge Alignment Problem: MixAlign",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2305.13669",
                            "summary": "Shows aligned clarifications markedly cut hallucinations but lacks broader benchmarks.",
                            "citation_reason": "Illustrates need for common evaluation across knowledge alignment tasks."
                        },
                        {
                            "title": "Revealing Personality Traits: PersonalityEvd Dataset",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2409.19723",
                            "summary": "Supplies evidence-based persona evaluation but still covers limited domains.",
                            "citation_reason": "Highlights current benchmarking progress yet remaining gaps."
                        }
                    ]
                }
            ]
        },
        {
            "number": 5,
            "title": "Future Directions",
            "outline": "Identify unresolved challenges and propose research avenues for more robust, privacy-preserving, and practically deployable personalised and aligned LLMs.",
            "key_points": [
                {
                    "text": "Develop privacy-preserving personalisation that balances compute constraints with secure handling of user embeddings and vocabularies.",
                    "papers": [
                        {
                            "title": "Now It Sounds Like You: Learning Personalized Vocabulary On Device",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2305.03584",
                            "summary": "Introduces an on-device OOV adapter minimising memory and latency.",
                            "citation_reason": "Points to the promise of secure, edge-only personalisation."
                        },
                        {
                            "title": "Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Advocates encrypted personal information and lightweight inference.",
                            "citation_reason": "Shows hardware-aware design indispensable for privacy."
                        }
                    ]
                },
                {
                    "text": "Scale multi-objective RLHF frameworks with interpretable reward decomposition and user-controllable trade-offs.",
                    "papers": [
                        {
                            "title": "Arithmetic Control of LLMs for Diverse User Preferences",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.18571",
                            "summary": "Enables arithmetic manipulation of reward vectors for user-tuned outputs.",
                            "citation_reason": "Signals pathway to fine-grained, user-side alignment knobs."
                        },
                        {
                            "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.12845",
                            "summary": "Adds MoE gating for context-aware objective selection.",
                            "citation_reason": "Addresses interpretability and scalability of multi-objective rewards."
                        }
                    ]
                },
                {
                    "text": "Create richer, standardised benchmarks and human-in-the-loop protocols to evaluate persona consistency, safety, and user satisfaction at scale.",
                    "papers": [
                        {
                            "title": "Quantifying the Persona Effect in LLM Simulations",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2402.10811",
                            "summary": "Shows current persona variables explain limited behaviour variance.",
                            "citation_reason": "Motivates more discriminative evaluation datasets."
                        },
                        {
                            "title": "Dialogue Language Model with Large-Scale Persona Data Engineering",
                            "year": 2025,
                            "url": "https://arxiv.org/abs/2412.09034",
                            "summary": "Generates large persona-dialogue corpora for robust training.",
                            "citation_reason": "Demonstrates data engineering as a route to better benchmarks."
                        }
                    ]
                },
                {
                    "text": "Extend alignment research to high-stakes, multimodal and domain-specific settings beyond text-only chat.",
                    "papers": [
                        {
                            "title": "Optimizing Autonomous Driving for Safety with LLM-Enhanced RLHF",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.04481",
                            "summary": "Aligns driving decisions via RLHF augmented with physiological feedback.",
                            "citation_reason": "Illustrates emerging multimodal, safety-critical alignment frontier."
                        },
                        {
                            "title": "Factual and Personalized Recommendations using Language Models and Reinforcement Learning",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2310.06176",
                            "summary": "Applies personalised alignment to recommendation narratives.",
                            "citation_reason": "Shows potential for cross-domain adoption of alignment techniques."
                        }
                    ]
                }
            ]
        }
    ]
}