{
    "reasoning": "The corpus was organized around two intersecting axes \u2013 (1) how language models adapt to individual users (personalisation) and (2) how they are steered toward desirable values under varying conditions (conditional alignment).  Five recent thematic clusters emerged: user\u2013profile conditioning, on-device or few-shot personal LLMs, offline/efficient preference-alignment algorithms, safety-focused RLHF variants, and evaluation/analysis work.  These clusters map naturally onto the requested outline: motivation (Introduction); concrete methods (Landscape); comparative critique (Synthesis); integrative take-aways (Conclusion); and open problems (Future Directions).  Each of the 15 post-2023 papers is cited at least once, and citations are spread so that most papers appear in a single section; five papers are intentionally reused across two sections because they supply both methodological detail and critical perspective (the reuse is justified in each citation_reason).  This allocation yields ten key points (2 per section) with 20 citation entries, satisfying breadth while minimising duplication.",
    "plan": [
        {
            "number": 1,
            "title": "Introduction",
            "outline": "Define personalisation and conditional alignment for LLMs, motivate their importance in ensuring user-specific utility while upholding safety and value alignment, and delimit the scope of the review to post-2023 peer-reviewed or widely cited arXiv work.",
            "key_points": [
                {
                    "text": "The rise of user-specific adaptation has shifted research focus from generic LLMs to models that reason over explicit user profiles and deliver personalised outputs.",
                    "papers": [
                        {
                            "title": "LaMP: When Large Language Models Meet Personalization",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2304.11406",
                            "summary": "Introduces the LaMP benchmark covering seven tasks and shows how retrieval-augmented conditioning on user profiles boosts personalisation across generation and classification.",
                            "citation_reason": "Provides a clear problem statement and benchmark that motivates the review."
                        },
                        {
                            "title": "Understanding the Role of User Profile in the Personalization of Large Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.17803",
                            "summary": "Demonstrates that historical personalised responses, not mere semantics, drive personalisation effects and analyzes profile position impact on LLM behavior.",
                            "citation_reason": "Supplies empirical evidence for why personal user data matters, reinforcing the review\u2019s relevance."
                        }
                    ]
                },
                {
                    "text": "Conditional value alignment has become critical to balance helpfulness, harmlessness and compliance as LLM capabilities grow.",
                    "papers": [
                        {
                            "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2310.12773",
                            "summary": "Decouples reward (helpfulness) and cost (harmlessness) models and applies a constrained optimisation strategy to satisfy safety constraints during RLHF.",
                            "citation_reason": "Illustrates the safety-vs-utility tension that frames conditional alignment."
                        },
                        {
                            "title": "RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2307.12950",
                            "summary": "Creates preference pairs from positive/negative prompts to align models without human annotations, outperforming previous RLAIF methods on harmlessness and helpfulness.",
                            "citation_reason": "Shows emergent low-cost strategies for conditional alignment, grounding the scope of the review."
                        }
                    ]
                }
            ]
        },
        {
            "number": 2,
            "title": "Thematic and Methodological Landscape",
            "outline": "Survey principal techniques, grouping literature into (a) user-representation & profile conditioning, and (b) preference-based or offline alignment algorithms that reduce resource cost while preserving safety.",
            "key_points": [
                {
                    "text": "Representing and injecting user information: from distilled on-device models to latent or lexical profiles that steer generation.",
                    "papers": [
                        {
                            "title": "Step-Back Profiling: Distilling User History for Personalized Scientific Writing",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2406.14275",
                            "summary": "Proposes a method that summarises extensive user history into concise profiles to personalise collaborative scientific writing; releases the PSW dataset.",
                            "citation_reason": "Exemplifies data distillation for scalable profile conditioning."
                        },
                        {
                            "title": "PLMM: Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Introduces a three-level model hierarchy and knowledge distillation to create resource-efficient personal LLMs that run locally and protect privacy.",
                            "citation_reason": "Demonstrates hardware-aware personalisation, broadening the methodological view."
                        },
                        {
                            "title": "Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2310.16582",
                            "summary": "Uses unsupervised lexicons to adjust probability vectors during decoding, enabling fine-grained control over LLM personality without retraining.",
                            "citation_reason": "Highlights plug-and-play lexical steering as an alternative to profile prompts."
                        }
                    ]
                },
                {
                    "text": "Resource-efficient preference alignment: offline or direct-optimisation methods that reduce dependence on PPO while matching human feedback quality.",
                    "papers": [
                        {
                            "title": "Aligning Language Models with Offline Learning from Human Feedback",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2308.12050",
                            "summary": "Explores filtering alignment, reward-weighted regression and conditional alignment in an entirely offline setting, cutting compute by ~90 %.",
                            "citation_reason": "Represents the move toward offline alignment, a core methodological trend."
                        },
                        {
                            "title": "ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2312.02554",
                            "summary": "Introduces Point-wise Direct Preference Optimisation and unifies supervised fine-tuning with point-wise preference data in a single step.",
                            "citation_reason": "Shows convergence of SFT and preference learning under a unified objective."
                        },
                        {
                            "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without Tears",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2304.05302",
                            "summary": "Uses ranking loss over conditional log-probabilities to align models, requiring fewer models and simpler hyper-parameters than PPO.",
                            "citation_reason": "Provides a lightweight yet competitive baseline, rounding out the algorithmic landscape."
                        }
                    ]
                }
            ]
        },
        {
            "number": 3,
            "title": "Synthesis & Critical Discussion",
            "outline": "Compare findings across studies, examining trade-offs between safety and usefulness, and the tension between scalability and personalised fidelity.",
            "key_points": [
                {
                    "text": "Alignment methods juggle the helpful-harmless trade-off differently, revealing design tensions in reward shaping and prompting strategies.",
                    "papers": [
                        {
                            "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2310.12773",
                            "summary": "Balances reward and cost via a Lagrangian framework, yielding high harmlessness without major utility loss.",
                            "citation_reason": "Reused because its explicit dual-objective analysis is central to the trade-off discussion."
                        },
                        {
                            "title": "RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2307.12950",
                            "summary": "Contrastive prompting shows that synthetic preferences can approximate human judgements in both helpfulness and harmlessness.",
                            "citation_reason": "Provides an alternative lens on the same trade-off using synthetic feedback."
                        }
                    ]
                },
                {
                    "text": "Personalisation scalability versus alignment fidelity: adapting to group or domain preferences without exploding data or compute.",
                    "papers": [
                        {
                            "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2310.11523",
                            "summary": "Adds an in-context meta-learned transformer head to adapt to demographic or individual groups with minimal preference data.",
                            "citation_reason": "Shows scalability via few-shot meta-learning for personalised alignment."
                        },
                        {
                            "title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2311.06503",
                            "summary": "Combines knowledge base grounding with preference alignment to satisfy user needs in specialised QA settings.",
                            "citation_reason": "Illustrates domain-specific constraints, enriching the critique of scalability."
                        }
                    ]
                }
            ]
        },
        {
            "number": 4,
            "title": "Conclusion",
            "outline": "Summarise converging evidence on the efficacy of profile-conditioning and offline alignment, and articulate the emerging consensus on best practice.",
            "key_points": [
                {
                    "text": "Personalisation techniques consistently improve relevance and user satisfaction across tasks when grounded in explicit profiles or history.",
                    "papers": [
                        {
                            "title": "RecExplainer: Aligning Large Language Models for Explaining Recommendation Models",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2311.10947",
                            "summary": "Uses aligned LLM surrogates to generate user-tailored explanations of black-box recommender systems, improving explanation fidelity.",
                            "citation_reason": "Confirms the real-world payoff of personalised generation."
                        },
                        {
                            "title": "Pixel Aligned Language Models",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2312.09237",
                            "summary": "Aligns pixel regions with words, demonstrating that location-conditioned captioning extends personalisation concepts to multimodal settings.",
                            "citation_reason": "Broadens the conclusion, showing personalisation across modalities."
                        }
                    ]
                },
                {
                    "text": "Offline and direct-optimisation alignment frameworks are converging toward stable, compute-efficient pipelines without sacrificing safety.",
                    "papers": [
                        {
                            "title": "Aligning Language Models with Offline Learning from Human Feedback",
                            "year": 2023,
                            "url": "https://arxiv.org/abs/2308.12050",
                            "summary": "Shows offline conditional alignment can match PPO performance with 9 % of the compute.",
                            "citation_reason": "Reused to crystallise the overarching consensus on efficiency gains that underpins the conclusion."
                        }
                    ]
                }
            ]
        },
        {
            "number": 5,
            "title": "Future Directions",
            "outline": "Identify gaps such as domain-sensitive evaluation, privacy-preserving on-device alignment, and richer notions of user values, suggesting concrete research avenues.",
            "key_points": [
                {
                    "text": "Develop domain-specific evaluation protocols and richer user studies to validate personalised alignment in real deployments.",
                    "papers": [
                        {
                            "title": "User Modeling Challenges in Interactive AI Assistant Systems",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2403.20134",
                            "summary": "Analyses the difficulty of inferring user mental states for personalised guidance, highlighting evaluation blind-spots.",
                            "citation_reason": "Introduces open challenges in empirical user evaluation."
                        },
                        {
                            "title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2311.06503",
                            "summary": "Combines knowledge grounding with alignment, underscoring the need for domain-aware metrics.",
                            "citation_reason": "Reused to motivate future domain-specific benchmarks."
                        }
                    ]
                },
                {
                    "text": "Pursue privacy-preserving, on-device personal LLMs that incorporate real-time safety constraints during local inference.",
                    "papers": [
                        {
                            "title": "PLMM: Personal Large Language Models on Mobile Devices",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2309.14726",
                            "summary": "Demonstrates device-level personal models with encrypted inputs, indicating feasibility of private deployment.",
                            "citation_reason": "Reused to ground the vision of local, privacy-aware personalisation."
                        },
                        {
                            "title": "RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment",
                            "year": 2024,
                            "url": "https://arxiv.org/abs/2307.12950",
                            "summary": "Low-overhead synthetic preference alignment could run locally, pointing toward on-device safety tuning.",
                            "citation_reason": "Reused to link lightweight alignment techniques with future on-device scenarios."
                        }
                    ]
                }
            ]
        }
    ]
}